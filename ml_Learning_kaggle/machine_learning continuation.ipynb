{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1e137d57bbe8b21aead32243cef5b3634f0f986d9a4d4c0f65dc1fd0f7137e9c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# --> We use data to decide how to break the houses into two groups, and then again to determine the predicted price in each group.   ------> This step of capturing patterns from data is called fitting or training the model. The data used to fit the model is called the training data.\n",
    "\n",
    "#deeper trees -> decision tree with more spits hence it is more complex \n",
    "\n",
    "#leaf_ml ---> the point at the bottom of a decision tree where the decision is made.\n",
    "\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "\n",
    "file_location = './data.csv'\n",
    "\n",
    "file_data = pd.read_csv(file_location)\n",
    "file_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"# --> using this code you get all the \n",
    "#file_d = file_data.dropna(axis = 0)\n",
    "#print(file_d)\n",
    "w = file_data.ZN[0]\n",
    "#--> usig this method you can select certain columns and analyse the data hence making this a very efficient way of working with data\n",
    "file_data_feature =['ZN','CRIM','CHAS']\n",
    "h = file_data[file_data_feature]\n",
    "#h.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----> building our model\n",
    "# in creating models we will use a library called scikit-learn, but it's written as sklearn\n",
    "# #scikit-learn is the most popular library for modeling the types of data typicaly stored in dataframes \n",
    "\n",
    "# the steps to building a model includes \n",
    "# --> DEFINE what type of model will it be? A decision tree or some other type of model? some other parameters of the model type are specified too.\n",
    "\n",
    "#------> FIT : Capture patterns from the provided data.\n",
    "# -----> PREDICT :: predict the data\n",
    "\n",
    "# -------> EVALUATE :: determine how accurrate the model is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"from sklearn.tree import DecisionTreeRegressor\\nimport pandas as pd\\n\\nmel_location ='./mel_data.csv'\\nmel_data = pd.read_csv(mel_location)\\nmel_data.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\\n#print(mel_data.columns)\\nfeature = ['Suburb', 'Address', 'Rooms', 'Type', 'Method', 'SellerG','Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',\\n       'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',\\n       'Longtitude', 'Regionname', 'Propertycount']\\n\\nx = mel_data[:len(mel_data)//2]\\nx_test = mel_data[len(mel_data)//2:]\\n\\n#training data\\nx_train = x[feature]\\n\\n#testing data\\nx_t = x_test[feature]\\n\\n#training outcomes\\ny_train = x['Price']\\nprint(y_train)\\n#del y_train[0],y_train[3]\\n\\n#print(y_train)\\nmel_data_model = DecisionTreeRegressor(random_state = 0)\\n\\nmel_data_model.fit(x_train,y_train)\\n\\nw = mel_data_model.predict(x_t)\\nprint(w)\""
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "'''from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "mel_location ='./mel_data.csv'\n",
    "mel_data = pd.read_csv(mel_location)\n",
    "mel_data.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "#print(mel_data.columns)\n",
    "feature = ['Suburb', 'Address', 'Rooms', 'Type', 'Method', 'SellerG','Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',\n",
    "       'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',\n",
    "       'Longtitude', 'Regionname', 'Propertycount']\n",
    "\n",
    "x = mel_data[:len(mel_data)//2]\n",
    "x_test = mel_data[len(mel_data)//2:]\n",
    "\n",
    "#training data\n",
    "x_train = x[feature]\n",
    "\n",
    "#testing data\n",
    "x_t = x_test[feature]\n",
    "\n",
    "#training outcomes\n",
    "y_train = x['Price']\n",
    "print(y_train)\n",
    "#del y_train[0],y_train[3]\n",
    "\n",
    "#print(y_train)\n",
    "mel_data_model = DecisionTreeRegressor(random_state = 0)\n",
    "\n",
    "mel_data_model.fit(x_train,y_train)\n",
    "\n",
    "w = mel_data_model.predict(x_t)\n",
    "print(w)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Underfitting and overfitting data \\nWhen we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes' actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses).\\n\\nThis is a phenomenon called overfitting,\\n\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "'''Underfitting and overfitting data \n",
    "When we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes' actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses).\n",
    "\n",
    "This is a phenomenon called overfitting,\n",
    "\n",
    "\n",
    "At an extreme, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data (and it will be bad in validation too for the same reason). When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called underfitting\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To solve this you have to determine the tree_depth that is best suited to give you accurate predictions and data.\n",
    "\n",
    "The Decision_tree_regressor comes with many parameters and allows you to chose what the tree depth is or the max_leaf_node\n",
    "\n",
    "And this allows you to control the problem of overfitting and underfitting\n",
    "\n",
    "he more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "\n",
    "'''\n",
    "using this method of comparing the mae of different max_leaf_nodes you can better fit your data to get more acurate predictions\n",
    "\n",
    "\n",
    "Max leaf nodes: 5  \t\t     Mean Absolute Error:  347380\n",
    "Max leaf nodes: 50  \t\t Mean Absolute Error:  258171\n",
    "Max leaf nodes: 500  \t\t Mean Absolute Error:  243495\n",
    "Max leaf nodes: 5000  \t\t Mean Absolute Error:  254983\n",
    "\n",
    "\n",
    "Overfitting: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or\n",
    "Underfitting: failing to capture relevant patterns, again leading to less accurate predictions.\n",
    "'''"
   ]
  }
 ]
}